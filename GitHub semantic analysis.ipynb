{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out names of cols. We can't read the whole file at once, because it has troubles with structure\n",
    "pre_fb = pd.read_csv('FA_data.csv', sep = ';', index_col = False, encoding='utf8', nrows = 10)\n",
    "cols = pre_fb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the whole file and change columns name\n",
    "f_fb = pd.read_csv('FA_data.csv', sep = ';', index_col = False, encoding='utf8', usecols = cols)\n",
    "\n",
    "f_fb.columns = ['Campaign name', 'Adset Name', 'Ad name', 'Creative text', 'Impressions', 'Clicks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Impressions to int and delete unsuitable entries \n",
    "for i in range(34413):\n",
    "    try:\n",
    "        int(f_fb['Impressions'][i])\n",
    "    except:\n",
    "        f_fb['Impressions'][i] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for Clicks\n",
    "for i in range(34413):\n",
    "    try:\n",
    "        int(f_fb['Clicks'][i])\n",
    "    except:\n",
    "        f_fb['Clicks'][i] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleted entries with NA\n",
    "fb = f_fb[f_fb['Impressions'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group our entries by ad text\n",
    "fb['Impressions'] = fb['Impressions'].astype(float)\n",
    "fb['Clicks'] = fb['Clicks'].astype(float)\n",
    "\n",
    "fb_g = fb.groupby('Creative text').agg({'Impressions':'sum','Clicks':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring the table to a convenient view\n",
    "fb_g['text'] = fb_g.index\n",
    "fb_g.index = range(len(fb_g))\n",
    "cols = ['text','Impressions', 'Clicks']\n",
    "fb_g = fb_g[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CTR\n",
    "fb_g['CTR'] = ''\n",
    "for i in fb_g.index:\n",
    "    fb_g['CTR'][i] = fb_g['Clicks'][i] / fb_g['Impressions'][i]\n",
    "fb_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our data to Excel to delete all entries not in English\n",
    "fb_g.to_excel('data_all_languages.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our data after cleaning\n",
    "fb_final = pd.read_excel('data_all_languages.xlsx', index_col = False, usecols = ['text','Impressions', 'Clicks', 'CTR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe our data\n",
    "fb_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize texts with High (1), Low (0) CTR (border is 0.006) \n",
    "fb_final['CTR cat'] = ''\n",
    "for i in range(546):\n",
    "    if fb_final['CTR'][i] >= 0.006:\n",
    "        fb_final['CTR cat'][i] = int(1)\n",
    "    elif fb_final['CTR'][i] <= 0.006:\n",
    "        fb_final['CTR cat'][i] = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for text cleaning\n",
    "def cleaner(text_string):\n",
    "    words = re.sub('[)(,.!?#]', ' ',text_string).replace(\"  \", ' ')\n",
    "    words2 = words.replace('  ', ' ')\n",
    "    words_split = re.split(' ', words2)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words_stemmed = ''\n",
    "    for word in words_split:\n",
    "            if re.sub(\"\\w{1,15}\", '', word) == '':\n",
    "                words_stemmed += stemmer.stem(word) + ' '\n",
    "            else:\n",
    "                words_stemmed += ''\n",
    "    words_stemmed = re.sub('\\s{2,10}', ' ', words_stemmed)\n",
    "    words_stemmed = re.sub('\\d{1,10}', '', words_stemmed)\n",
    "    \n",
    "    return words_stemmed.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_clean = fb_final\n",
    "fb_clean['text'] = fb_clean['text'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering our texts to more accurately divide them into training and testing samples\n",
    "#!!! if one of the clusters has only 1 text, repeat clustering and only then upload it to csv\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=7, init='k-means++', max_iter=10000)\n",
    "km.fit(X)\n",
    "fb_clean['label'] = km.labels_\n",
    "print(fb_clean.groupby('label').agg({'CTR':'mean', 'text':'count'}))\n",
    "print(fb_clean.groupby('label').agg({'CTR':'std'}))\n",
    "fb_clean.to_csv('fin_fb_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of stop_words, which don't have semantic load and will be excluded from our analysis  \n",
    "sw = ['the', 'to', 'and', 'of', 'you', 'your', 'on', 'for', 'in', 'it', 'is', 'are', 'how', 'with',\n",
    "     'from', 'if', 'this', 'at', 'can', 'or', 'if', 'so', 'what', 'do', 'got', 'more', 'out', 'all',\n",
    "     'our', 'one', 'them', 'that', 'be', 'thing', 'yet', 'an', 'by', 'have', 'few', 'we', 'end', 'let', 'averag',\n",
    "     'has', 'not', 'recent', 'us', 'off', 'no', 'as', 'but', 'they', 'their', 'new', 'whi', 'than']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize our texts \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=True, stop_words=sw, min_df = 0.03)\n",
    "cv.fit(fb_clean['text'])\n",
    "X = cv.transform(fb_clean['text'])\n",
    "Y = fb_clean['CTR cat']\n",
    "Y = Y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 5 folds for cross_val_score \n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a logistic regression model and look at words with highest and lowest coeficients\n",
    "fb_label = pd.read_csv('fin_fb_clean.csv')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, train_size = 0.8, stratify = fb_label['label'], \n",
    "                                                  random_state = 1212)\n",
    "\n",
    "\n",
    "lr = LogisticRegression(multi_class= 'auto', C=0.09)\n",
    "lr.fit(X_train, y_train)\n",
    "print('')\n",
    "print (\"Accuracy =\", accuracy_score(y_val, lr.predict(X_val)))\n",
    "print('')\n",
    "print('Cross_val_score', cross_val_score(lr, X, Y, cv=kf, scoring=('accuracy')).mean())\n",
    "print()\n",
    "print(classification_report(y_val, lr.predict(X_val)))\n",
    "\n",
    "\n",
    " \n",
    "#sanity check - let's look at the top features\n",
    "#a function that fetches the feature name from vectorizer using the classifier coefficients\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), lr.coef_[0]\n",
    "    )\n",
    "}\n",
    "#Now let's print those features from the top of the list\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True)[:10]:\n",
    "    print('best positive', best_positive)\n",
    "#And from the bottom of the list\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(),\n",
    "    key=lambda x: x[1])[:10]:\n",
    "    print('best negative', best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect words with highest and lowest coefficients for further analysis\n",
    "key_words_new = []\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), lr.coef_[0]\n",
    "    )\n",
    "}\n",
    "\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True)[:10]:\n",
    "    key_words_new.append(best_positive)\n",
    "\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(),\n",
    "    key=lambda x: x[1])[:10]:\n",
    "    key_words_new.append(best_negative)\n",
    "    \n",
    "key_words_2 = []\n",
    "\n",
    "for word in key_words_new: \n",
    "    key_words_2.append(str(word).split(\"'\")[1])\n",
    "\n",
    "key_words_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a final table\n",
    "for word in key_words_2:\n",
    "    fb_clean[word] = ''\n",
    "    fb_clean[word] = fb_clean['text'].apply(lambda text:1 if word in text else 0 )\n",
    "    \n",
    "d = {'word': key_words_2}\n",
    "table = pd.DataFrame(d)\n",
    "\n",
    "table['median CTR %'] = ''\n",
    "for i in range(len(table)):\n",
    "    table['median CTR %'][i] = round(fb_clean['CTR'][fb_clean[table['word'][i]] == 1].median() * 100, 2)\n",
    "    \n",
    "table['num of texts with word'] = ''\n",
    "for i in range(len(table)):\n",
    "    table['num of texts with word'][i] = len(fb_clean[fb_clean[table['word'][i]] == 1])\n",
    "    \n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import table to Excel for further analysis\n",
    "table.to_excel('CTR_006.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###\n",
    "###\n",
    "### Now we build another model with another border of CTR\n",
    "###\n",
    "###\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize texts with High (1), Low (0) CTR (border is 0.01) \n",
    "fb_final['CTR cat'] = ''\n",
    "for i in range(546):\n",
    "    if fb_final['CTR'][i] >= 0.01:\n",
    "        fb_final['CTR cat'][i] = int(1)\n",
    "    elif fb_final['CTR'][i] <= 0.01:\n",
    "        fb_final['CTR cat'][i] = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning function\n",
    "fb_clean = fb_final\n",
    "fb_clean['text'] = fb_clean['text'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize our texts \n",
    "cv = CountVectorizer(binary=True, stop_words=sw)\n",
    "cv.fit(fb_clean['text'])\n",
    "X = cv.transform(fb_clean['text'])\n",
    "Y = fb_clean['CTR cat']\n",
    "Y = Y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a logistic regression model and look at words with highest and lowest coeficients\n",
    "fb_label = pd.read_csv('fin_fb_clean.csv')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, train_size = 0.8, stratify = fb_label['label'], \n",
    "                                                  random_state = 777)\n",
    "\n",
    "\n",
    "lr = LogisticRegression(multi_class= 'auto', C=0.5)\n",
    "lr.fit(X_train, y_train)\n",
    "print('')\n",
    "print (\"Accuracy =\", accuracy_score(y_val, lr.predict(X_val)))\n",
    "print('')\n",
    "print('Cross_val_score', cross_val_score(lr, X, Y, cv=kf, scoring=('accuracy')).mean())\n",
    "print()\n",
    "print(classification_report(y_val, lr.predict(X_val)))\n",
    "\n",
    "\n",
    " \n",
    "#sanity check - let's look at the top features\n",
    "#a function that fetches the feature name from vectorizer using the classifier coefficients\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), lr.coef_[0]\n",
    "    )\n",
    "}\n",
    "#Now let's print those features from the top of the list\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True)[:10]:\n",
    "    print('best positive', best_positive)\n",
    "#And from the bottom of the list\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(),\n",
    "    key=lambda x: x[1])[:10]:\n",
    "    print('best negative', best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect words with highest and lowest coefficients for further analysis\n",
    "key_words_new = []\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), lr.coef_[0]\n",
    "    )\n",
    "}\n",
    "\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True)[:10]:\n",
    "    key_words_new.append(best_positive)\n",
    "\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(),\n",
    "    key=lambda x: x[1])[:10]:\n",
    "    key_words_new.append(best_negative)\n",
    "    \n",
    "key_words_2 = []\n",
    "\n",
    "for word in key_words_new: \n",
    "    key_words_2.append(str(word).split(\"'\")[1])\n",
    "\n",
    "key_words_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a final table\n",
    "for word in key_words_2:\n",
    "    fb_clean[word] = ''\n",
    "    fb_clean[word] = fb_clean['text'].apply(lambda text:1 if word in text else 0 )\n",
    "    \n",
    "d = {'word': key_words_2}\n",
    "table = pd.DataFrame(d)\n",
    "\n",
    "table['median CTR %'] = ''\n",
    "for i in range(len(table)):\n",
    "    table['median CTR %'][i] = round(fb_clean['CTR'][fb_clean[table['word'][i]] == 1].median() * 100, 2)\n",
    "    \n",
    "table['num of texts with word'] = ''\n",
    "for i in range(len(table)):\n",
    "    table['num of texts with word'][i] = len(fb_clean[fb_clean[table['word'][i]] == 1])\n",
    "    \n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import table to Excel for further analysis\n",
    "table.to_excel('CTR_01.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
